---
title: 正则化范数理解
date: 2016-10-24
layout: post
categories:
 - 机器学习
---
# 前提说明
正则化是机器学习中很重要的减轻过拟合的手段，在loss函数后面加上衡量模型复杂度的相关范数就能防止模型变得过于复杂。经常碰到类似L0、L1、L2范数如何区别的问题。对于非数学专业背景的数学功底辣鸡的我来说，只能是死记住这些范数的含义，达到了真正的知其然而不知其所以然的“境界”...

来来来，跟我一起记一下：
L0、L1范数会使得模型对应matrix变得稀疏。L0是代表模型的参数matrix中非零元素的个数。L1代表各个元素中绝对值的和，回归模型里用到它就可以叫做Lasso回归，使用L0和L1均可以实现“稀疏”的效果
>举一例在线性回归（LR）中假设某一有效特征$$x_i$$对应的权值为$$w_i=a$$那么当复制一份$$x_i$$为$$x_j$$时最终训练出来的结果$$w_i$$和$$w_j$$的权重倾向一个保持$$a$$另一个变为0（L2倾向于两者变成$$a/2$$）。
由于L1相比于L0在数学上有“更好的性质”（我的理解：因为数零个数很难用一个没有分支的语句给表达出来），因此两者均可的时候往往选择L1。L2范数使得模型对应的参数matrix更加均衡

这些知识是从**参考材料1**中得到的。这样记下来的知识点有很容易忘记的缺点，因为他们在我的生物神经网络中不再一个体系中。有没有L-1范数或者L3、L4范数？为什么？类似与这样的问题我是完全没有思路的。
读到**DeepLearningBook**的数学预备部分时看到了[2.5 Norms]部分时看到了如下公式：
$$||x||_p = \big{\{}\sum_i|x_i|^p\big{\}}^{1/p}$$

# 参考材料
[1][机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995/)

[2][DeepLearningBook](https://github.com/HFTrader/DeepLearningBook/blob/master/DeepLearningBook.pdf)
 
