---
title: 对正则化中范数的理解
date: 2016-10-24
layout: post
comments: true
categories:
 - 机器学习
---
# 前提说明
正则化是机器学习中很重要的减轻过拟合的手段，在loss函数后面加上衡量模型复杂度的相关范数就能防止模型变得过于复杂。经常碰到类似L0、L1、L2范数如何区别的问题。对于非数学专业背景的数学功底辣鸡的我来说，只能是死记住这些范数的含义，达到了真正的知其然而不知其所以然的“境界”...

来来来，跟我一起记一下：
L0、L1范数会使得模型对应vector变得稀疏。L0是代表模型的参数vector中非零元素的个数。L1代表各个元素中绝对值的和，回归模型里用到它就可以叫做Lasso回归，使用L0和L1均可以实现“稀疏”的效果。

> 举一例在线性回归（LR）中假设某一有效特征$$x_i$$对应的权值为$$w_i=a$$那么当复制一份$$x_i$$为$$x_j$$时最终训练出来的结果$$w_i$$和$$w_j$$的权重倾向一个保持$$a$$另一个变为0（L2倾向于两者变成$$a/2$$）。
由于L1相比于L0在数学上有“更好的性质”（我的理解：因为数零个数很难用一个没有分支的语句给表达出来），因此两者均可的时候往往选择L1。L2范数使得模型对应的参数vector更加均衡。

# norms理解
上面这些知识是通过互联网材料获取得到的，典型的一篇为**参考材料1**。这样获取的的知识点有很容易忘记的缺点，因为它们在我的生物神经网络中不在一个体系中。有没有L-1范数或者L3、L4范数？为什么？类似与这样的问题我是完全没有思路的。

最近读到**DeepLearningBook**的数学预备部分（2.5节）时看到了Norms的数学定义公式：

$$||x||_p = \big{\{}\sum_i|x_i|^p\big{\}}^{1/p} (p \geq 1)$$

这个公式是博主第一次见到，理解了这个公式对于我如何记忆L0、L1、L2范数是有极大帮助的，这里$$p$$分别取值1、2就对应了L1和L2范数。

当$p$为0的时候碰到的第一个麻烦是基本运算无法将$$target=\left\{\begin{array}{ll}1&\text{$x \!eq 0$},\\0&\text{$x = 0$}.\end{array}\right.$$表达出来。这就给后期的公式的表达和数学计算带来了一定的难度。接着L0范数违反了norm性质的第三条$$任意\alpha \in R, f(\alpha x) = |\alpha|f(x)$$。
因此严格来讲L0范数虽然被口头上熟知但是L0对应的norm确实不存在的一个概念。

# 参考材料
[1][机器学习中的范数规则化之（一）L0、L1与L2范数](http://blog.csdn.net/zouxy09/article/details/24971995/)

[2][DeepLearningBook](https://github.com/HFTrader/DeepLearningBook/blob/master/DeepLearningBook.pdf)
 
